{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... loading data\n",
      "... building the model\n",
      "... getting the pretraining functions\n",
      "... pre-training the model\n",
      "Pre-training layer 0, epoch 0, cost  -98.5364928294\n",
      "Pre-training layer 0, epoch 1, cost  -83.8445576167\n",
      "Pre-training layer 0, epoch 2, cost  -80.6960564003\n",
      "Pre-training layer 0, epoch 3, cost  -79.0383977598\n",
      "Pre-training layer 0, epoch 4, cost  -77.9289304769\n",
      "Pre-training layer 0, epoch 5, cost  -77.0886402509\n",
      "Pre-training layer 0, epoch 6, cost  -76.4052954437\n",
      "Pre-training layer 0, epoch 7, cost  -75.8301243622\n",
      "Pre-training layer 0, epoch 8, cost  -75.3491888991\n",
      "Pre-training layer 0, epoch 9, cost  -74.9350097439\n",
      "Pre-training layer 0, epoch 10, cost  -74.5998971206\n",
      "Pre-training layer 0, epoch 11, cost  -74.2626780101\n",
      "Pre-training layer 0, epoch 12, cost  -73.9251497428\n",
      "Pre-training layer 0, epoch 13, cost  -73.6783177032\n",
      "Pre-training layer 0, epoch 14, cost  -73.4248720327\n",
      "Pre-training layer 0, epoch 15, cost  -73.1633478413\n",
      "Pre-training layer 0, epoch 16, cost  -72.9604901037\n",
      "Pre-training layer 0, epoch 17, cost  -72.7854133369\n",
      "Pre-training layer 0, epoch 18, cost  -72.6088669547\n",
      "Pre-training layer 0, epoch 19, cost  -72.4559645729\n",
      "Pre-training layer 0, epoch 20, cost  -72.2638562163\n",
      "Pre-training layer 0, epoch 21, cost  -72.1616614222\n",
      "Pre-training layer 0, epoch 22, cost  -72.0182901338\n",
      "Pre-training layer 0, epoch 23, cost  -71.8241394389\n",
      "Pre-training layer 0, epoch 24, cost  -71.7126862989\n",
      "Pre-training layer 0, epoch 25, cost  -71.5770658302\n",
      "Pre-training layer 0, epoch 26, cost  -71.4527997993\n",
      "Pre-training layer 0, epoch 27, cost  -71.3674882287\n",
      "Pre-training layer 0, epoch 28, cost  -71.2364661364\n",
      "Pre-training layer 0, epoch 29, cost  -71.1644354783\n",
      "Pre-training layer 0, epoch 30, cost  -71.0075229989\n",
      "Pre-training layer 0, epoch 31, cost  -70.9571400144\n",
      "Pre-training layer 0, epoch 32, cost  -70.8523582766\n",
      "Pre-training layer 0, epoch 33, cost  -70.7868064073\n",
      "Pre-training layer 0, epoch 34, cost  -70.7025731038\n",
      "Pre-training layer 0, epoch 35, cost  -70.6155044823\n",
      "Pre-training layer 0, epoch 36, cost  -70.5568279543\n",
      "Pre-training layer 0, epoch 37, cost  -70.4573152786\n",
      "Pre-training layer 0, epoch 38, cost  -70.3969058688\n",
      "Pre-training layer 0, epoch 39, cost  -70.3253985774\n",
      "Pre-training layer 0, epoch 40, cost  -70.2414306311\n",
      "Pre-training layer 0, epoch 41, cost  -70.1585990567\n",
      "Pre-training layer 0, epoch 42, cost  -70.110838961\n",
      "Pre-training layer 0, epoch 43, cost  -70.0653162998\n",
      "Pre-training layer 0, epoch 44, cost  -69.9996359599\n",
      "Pre-training layer 0, epoch 45, cost  -69.982644646\n",
      "Pre-training layer 0, epoch 46, cost  -69.8997687771\n",
      "Pre-training layer 0, epoch 47, cost  -69.8454536232\n",
      "Pre-training layer 0, epoch 48, cost  -69.7889215087\n",
      "Pre-training layer 0, epoch 49, cost  -69.7375897864\n",
      "Pre-training layer 0, epoch 50, cost  -69.7017213958\n",
      "Pre-training layer 0, epoch 51, cost  -69.6970014216\n",
      "Pre-training layer 0, epoch 52, cost  -69.6254285625\n",
      "Pre-training layer 0, epoch 53, cost  -69.5636148947\n",
      "Pre-training layer 0, epoch 54, cost  -69.5459814966\n",
      "Pre-training layer 0, epoch 55, cost  -69.485764992\n",
      "Pre-training layer 0, epoch 56, cost  -69.4696184928\n",
      "Pre-training layer 0, epoch 57, cost  -69.4381588818\n",
      "Pre-training layer 0, epoch 58, cost  -69.3834019843\n",
      "Pre-training layer 0, epoch 59, cost  -69.3728249318\n",
      "Pre-training layer 0, epoch 60, cost  -69.3456471013\n",
      "Pre-training layer 0, epoch 61, cost  -69.2961143992\n",
      "Pre-training layer 0, epoch 62, cost  -69.2652850209\n",
      "Pre-training layer 0, epoch 63, cost  -69.230027995\n",
      "Pre-training layer 0, epoch 64, cost  -69.215559116\n",
      "Pre-training layer 0, epoch 65, cost  -69.1925345468\n",
      "Pre-training layer 0, epoch 66, cost  -69.1614099932\n",
      "Pre-training layer 0, epoch 67, cost  -69.136137801\n",
      "Pre-training layer 0, epoch 68, cost  -69.0873864585\n",
      "Pre-training layer 0, epoch 69, cost  -69.0835293461\n",
      "Pre-training layer 0, epoch 70, cost  -69.0362723294\n",
      "Pre-training layer 0, epoch 71, cost  -69.0215765778\n",
      "Pre-training layer 0, epoch 72, cost  -68.9968807965\n",
      "Pre-training layer 0, epoch 73, cost  -68.9977008851\n",
      "Pre-training layer 0, epoch 74, cost  -68.9500012494\n",
      "Pre-training layer 0, epoch 75, cost  -68.9082500451\n",
      "Pre-training layer 0, epoch 76, cost  -68.925425267\n",
      "Pre-training layer 0, epoch 77, cost  -68.9199095424\n",
      "Pre-training layer 0, epoch 78, cost  -68.8543313568\n",
      "Pre-training layer 0, epoch 79, cost  -68.8382676217\n",
      "Pre-training layer 0, epoch 80, cost  -68.8086835994\n",
      "Pre-training layer 0, epoch 81, cost  -68.8343672985\n",
      "Pre-training layer 0, epoch 82, cost  -68.7916243709\n",
      "Pre-training layer 0, epoch 83, cost  -68.7641604607\n",
      "Pre-training layer 0, epoch 84, cost  -68.7922705864\n",
      "Pre-training layer 0, epoch 85, cost  -68.7547982216\n",
      "Pre-training layer 0, epoch 86, cost  -68.718270653\n",
      "Pre-training layer 0, epoch 87, cost  -68.7305563765\n",
      "Pre-training layer 0, epoch 88, cost  -68.6993901299\n",
      "Pre-training layer 0, epoch 89, cost  -68.6868373982\n",
      "Pre-training layer 0, epoch 90, cost  -68.6784215524\n",
      "Pre-training layer 0, epoch 91, cost  -68.6555949681\n",
      "Pre-training layer 0, epoch 92, cost  -68.6458439819\n",
      "Pre-training layer 0, epoch 93, cost  -68.6140696241\n",
      "Pre-training layer 0, epoch 94, cost  -68.5847694018\n",
      "Pre-training layer 0, epoch 95, cost  -68.5833549534\n",
      "Pre-training layer 0, epoch 96, cost  -68.5572058568\n",
      "Pre-training layer 0, epoch 97, cost  -68.5706492398\n",
      "Pre-training layer 0, epoch 98, cost  -68.5181957705\n",
      "Pre-training layer 0, epoch 99, cost  -68.5266283308\n",
      "Pre-training layer 1, epoch 0, cost  -172.870613598\n",
      "Pre-training layer 1, epoch 1, cost  -150.62030588\n",
      "Pre-training layer 1, epoch 2, cost  -145.507106124\n",
      "Pre-training layer 1, epoch 3, cost  -142.714502875\n",
      "Pre-training layer 1, epoch 4, cost  -140.864057023\n",
      "Pre-training layer 1, epoch 5, cost  -139.459728895\n",
      "Pre-training layer 1, epoch 6, cost  -138.340679797\n",
      "Pre-training layer 1, epoch 7, cost  -137.429587033\n",
      "Pre-training layer 1, epoch 8, cost  -136.644563504\n",
      "Pre-training layer 1, epoch 9, cost  -135.968895898\n",
      "Pre-training layer 1, epoch 10, cost  -135.406811828\n",
      "Pre-training layer 1, epoch 11, cost  -134.915879594\n",
      "Pre-training layer 1, epoch 12, cost  -134.464076104\n",
      "Pre-training layer 1, epoch 13, cost  -134.085598628\n",
      "Pre-training layer 1, epoch 14, cost  -133.758519575\n",
      "Pre-training layer 1, epoch 15, cost  -133.443976485\n",
      "Pre-training layer 1, epoch 16, cost  -133.154721216\n",
      "Pre-training layer 1, epoch 17, cost  -132.937458503\n",
      "Pre-training layer 1, epoch 18, cost  -132.707429934\n",
      "Pre-training layer 1, epoch 19, cost  -132.480561572\n",
      "Pre-training layer 1, epoch 20, cost  -132.321968403\n",
      "Pre-training layer 1, epoch 21, cost  -132.17615947\n",
      "Pre-training layer 1, epoch 22, cost  -132.002580479\n",
      "Pre-training layer 1, epoch 23, cost  -131.842724384\n",
      "Pre-training layer 1, epoch 24, cost  -131.733950052\n",
      "Pre-training layer 1, epoch 25, cost  -131.617596827\n",
      "Pre-training layer 1, epoch 26, cost  -131.486905797\n",
      "Pre-training layer 1, epoch 27, cost  -131.389688195\n",
      "Pre-training layer 1, epoch 28, cost  -131.306791888\n",
      "Pre-training layer 1, epoch 29, cost  -131.193088768\n",
      "Pre-training layer 1, epoch 30, cost  -131.130524921\n",
      "Pre-training layer 1, epoch 31, cost  -131.032241448\n",
      "Pre-training layer 1, epoch 32, cost  -130.971844728\n",
      "Pre-training layer 1, epoch 33, cost  -130.902578352\n",
      "Pre-training layer 1, epoch 34, cost  -130.840296835\n",
      "Pre-training layer 1, epoch 35, cost  -130.771291361\n",
      "Pre-training layer 1, epoch 36, cost  -130.683790095\n",
      "Pre-training layer 1, epoch 37, cost  -130.661411366\n",
      "Pre-training layer 1, epoch 38, cost  -130.624820604\n",
      "Pre-training layer 1, epoch 39, cost  -130.567756773\n",
      "Pre-training layer 1, epoch 40, cost  -130.486454877\n",
      "Pre-training layer 1, epoch 41, cost  -130.44971701\n",
      "Pre-training layer 1, epoch 42, cost  -130.436305569\n",
      "Pre-training layer 1, epoch 43, cost  -130.367999331\n",
      "Pre-training layer 1, epoch 44, cost  -130.35644658\n",
      "Pre-training layer 1, epoch 45, cost  -130.299202045\n",
      "Pre-training layer 1, epoch 46, cost  -130.286072237\n",
      "Pre-training layer 1, epoch 47, cost  -130.262716879\n",
      "Pre-training layer 1, epoch 48, cost  -130.229322886\n",
      "Pre-training layer 1, epoch 49, cost  -130.187180244\n",
      "Pre-training layer 1, epoch 50, cost  -130.171222404\n",
      "Pre-training layer 1, epoch 51, cost  -130.141376211\n",
      "Pre-training layer 1, epoch 52, cost  -130.108470048\n",
      "Pre-training layer 1, epoch 53, cost  -130.0803823\n",
      "Pre-training layer 1, epoch 54, cost  -130.063007697\n",
      "Pre-training layer 1, epoch 55, cost  -130.023570251\n",
      "Pre-training layer 1, epoch 56, cost  -130.013558907\n",
      "Pre-training layer 1, epoch 57, cost  -130.001543892\n",
      "Pre-training layer 1, epoch 58, cost  -129.985487647\n",
      "Pre-training layer 1, epoch 59, cost  -129.964047226\n",
      "Pre-training layer 1, epoch 60, cost  -129.939379239\n",
      "Pre-training layer 1, epoch 61, cost  -129.916233493\n",
      "Pre-training layer 1, epoch 62, cost  -129.888044104\n",
      "Pre-training layer 1, epoch 63, cost  -129.896213503\n",
      "Pre-training layer 1, epoch 64, cost  -129.858416977\n",
      "Pre-training layer 1, epoch 65, cost  -129.859922408\n",
      "Pre-training layer 1, epoch 66, cost  -129.816057504\n",
      "Pre-training layer 1, epoch 67, cost  -129.792978746\n",
      "Pre-training layer 1, epoch 68, cost  -129.799364896\n",
      "Pre-training layer 1, epoch 69, cost  -129.797172885\n",
      "Pre-training layer 1, epoch 70, cost  -129.780034023\n",
      "Pre-training layer 1, epoch 71, cost  -129.769577969\n",
      "Pre-training layer 1, epoch 72, cost  -129.738772989\n",
      "Pre-training layer 1, epoch 73, cost  -129.717085401\n",
      "Pre-training layer 1, epoch 74, cost  -129.723561881\n",
      "Pre-training layer 1, epoch 75, cost  -129.718097165\n",
      "Pre-training layer 1, epoch 76, cost  -129.698606321\n",
      "Pre-training layer 1, epoch 77, cost  -129.692817291\n",
      "Pre-training layer 1, epoch 78, cost  -129.695457988\n",
      "Pre-training layer 1, epoch 79, cost  -129.67702867\n",
      "Pre-training layer 1, epoch 80, cost  -129.661061984\n",
      "Pre-training layer 1, epoch 81, cost  -129.642895211\n",
      "Pre-training layer 1, epoch 82, cost  -129.638795824\n",
      "Pre-training layer 1, epoch 83, cost  -129.614324324\n",
      "Pre-training layer 1, epoch 84, cost  -129.604274027\n",
      "Pre-training layer 1, epoch 85, cost  -129.626295989\n",
      "Pre-training layer 1, epoch 86, cost  -129.610462685\n",
      "Pre-training layer 1, epoch 87, cost  -129.624390877\n",
      "Pre-training layer 1, epoch 88, cost  -129.610562563\n",
      "Pre-training layer 1, epoch 89, cost  -129.605955023\n",
      "Pre-training layer 1, epoch 90, cost  -129.562344634\n",
      "Pre-training layer 1, epoch 91, cost  -129.559810814\n",
      "Pre-training layer 1, epoch 92, cost  -129.565971641\n",
      "Pre-training layer 1, epoch 93, cost  -129.550156799\n",
      "Pre-training layer 1, epoch 94, cost  -129.53509213\n",
      "Pre-training layer 1, epoch 95, cost  -129.53829016\n",
      "Pre-training layer 1, epoch 96, cost  -129.521215234\n",
      "Pre-training layer 1, epoch 97, cost  -129.531672581\n",
      "Pre-training layer 1, epoch 98, cost  -129.542776283\n",
      "Pre-training layer 1, epoch 99, cost  -129.499918026\n",
      "Pre-training layer 2, epoch 0, cost  -68.7642154812\n",
      "Pre-training layer 2, epoch 1, cost  -56.6083767947\n",
      "Pre-training layer 2, epoch 2, cost  -54.1713965818\n",
      "Pre-training layer 2, epoch 3, cost  -52.8785082098\n",
      "Pre-training layer 2, epoch 4, cost  -52.0188454496\n",
      "Pre-training layer 2, epoch 5, cost  -51.4112387317\n",
      "Pre-training layer 2, epoch 6, cost  -50.8869870922\n",
      "Pre-training layer 2, epoch 7, cost  -50.5028185006\n",
      "Pre-training layer 2, epoch 8, cost  -50.1487919638\n",
      "Pre-training layer 2, epoch 9, cost  -49.8547080828\n",
      "Pre-training layer 2, epoch 10, cost  -49.5833876409\n",
      "Pre-training layer 2, epoch 11, cost  -49.3617407339\n",
      "Pre-training layer 2, epoch 12, cost  -49.1383299068\n",
      "Pre-training layer 2, epoch 13, cost  -48.9600040213\n",
      "Pre-training layer 2, epoch 14, cost  -48.7940702753\n",
      "Pre-training layer 2, epoch 15, cost  -48.6348491021\n",
      "Pre-training layer 2, epoch 16, cost  -48.4847234837\n",
      "Pre-training layer 2, epoch 17, cost  -48.3693940076\n",
      "Pre-training layer 2, epoch 18, cost  -48.2528012108\n",
      "Pre-training layer 2, epoch 19, cost  -48.1191015656\n",
      "Pre-training layer 2, epoch 20, cost  -48.0188623952\n",
      "Pre-training layer 2, epoch 21, cost  -47.9236639718\n",
      "Pre-training layer 2, epoch 22, cost  -47.8420560919\n",
      "Pre-training layer 2, epoch 23, cost  -47.7447405554\n",
      "Pre-training layer 2, epoch 24, cost  -47.6651204481\n",
      "Pre-training layer 2, epoch 25, cost  -47.5831379068\n",
      "Pre-training layer 2, epoch 26, cost  -47.5032491851\n",
      "Pre-training layer 2, epoch 27, cost  -47.4443080096\n",
      "Pre-training layer 2, epoch 28, cost  -47.3707275709\n",
      "Pre-training layer 2, epoch 29, cost  -47.3111164655\n",
      "Pre-training layer 2, epoch 30, cost  -47.2467491559\n",
      "Pre-training layer 2, epoch 31, cost  -47.179849165\n",
      "Pre-training layer 2, epoch 32, cost  -47.1431741767\n",
      "Pre-training layer 2, epoch 33, cost  -47.0949567074\n",
      "Pre-training layer 2, epoch 34, cost  -47.0379160354\n",
      "Pre-training layer 2, epoch 35, cost  -46.974187479\n",
      "Pre-training layer 2, epoch 36, cost  -46.9224097816\n",
      "Pre-training layer 2, epoch 37, cost  -46.901976594\n",
      "Pre-training layer 2, epoch 38, cost  -46.8205345385\n",
      "Pre-training layer 2, epoch 39, cost  -46.7926967823\n",
      "Pre-training layer 2, epoch 40, cost  -46.7804667564\n",
      "Pre-training layer 2, epoch 41, cost  -46.7175636961\n",
      "Pre-training layer 2, epoch 42, cost  -46.6917178767\n",
      "Pre-training layer 2, epoch 43, cost  -46.6397578693\n",
      "Pre-training layer 2, epoch 44, cost  -46.5996422948\n",
      "Pre-training layer 2, epoch 45, cost  -46.564459359\n",
      "Pre-training layer 2, epoch 46, cost  -46.5430225101\n",
      "Pre-training layer 2, epoch 47, cost  -46.5210993169\n",
      "Pre-training layer 2, epoch 48, cost  -46.4882485039\n",
      "Pre-training layer 2, epoch 49, cost  -46.4596858795\n",
      "Pre-training layer 2, epoch 50, cost  -46.41226555\n",
      "Pre-training layer 2, epoch 51, cost  -46.3903064335\n",
      "Pre-training layer 2, epoch 52, cost  -46.3631394375\n",
      "Pre-training layer 2, epoch 53, cost  -46.339597889\n",
      "Pre-training layer 2, epoch 54, cost  -46.3072616893\n",
      "Pre-training layer 2, epoch 55, cost  -46.3178923022\n",
      "Pre-training layer 2, epoch 56, cost  -46.271177232\n",
      "Pre-training layer 2, epoch 57, cost  -46.248652063\n",
      "Pre-training layer 2, epoch 58, cost  -46.2435762843\n",
      "Pre-training layer 2, epoch 59, cost  -46.2096129157\n",
      "Pre-training layer 2, epoch 60, cost  -46.2105134721\n",
      "Pre-training layer 2, epoch 61, cost  -46.173136222\n",
      "Pre-training layer 2, epoch 62, cost  -46.1504736666\n",
      "Pre-training layer 2, epoch 63, cost  -46.1332581697\n",
      "Pre-training layer 2, epoch 64, cost  -46.1139154584\n",
      "Pre-training layer 2, epoch 65, cost  -46.086065932\n",
      "Pre-training layer 2, epoch 66, cost  -46.1026206916\n",
      "Pre-training layer 2, epoch 67, cost  -46.071968228\n",
      "Pre-training layer 2, epoch 68, cost  -46.0498316521\n",
      "Pre-training layer 2, epoch 69, cost  -46.0345114684\n",
      "Pre-training layer 2, epoch 70, cost  -46.0193890718\n",
      "Pre-training layer 2, epoch 71, cost  -45.9884968936\n",
      "Pre-training layer 2, epoch 72, cost  -45.9895023417\n",
      "Pre-training layer 2, epoch 73, cost  -45.9759743779\n",
      "Pre-training layer 2, epoch 74, cost  -45.9820891613\n",
      "Pre-training layer 2, epoch 75, cost  -45.94619642\n",
      "Pre-training layer 2, epoch 76, cost  -45.9369964935\n",
      "Pre-training layer 2, epoch 77, cost  -45.9287105253\n",
      "Pre-training layer 2, epoch 78, cost  -45.9212151758\n",
      "Pre-training layer 2, epoch 79, cost  -45.8944820628\n",
      "Pre-training layer 2, epoch 80, cost  -45.8817458843\n",
      "Pre-training layer 2, epoch 81, cost  -45.8761749656\n",
      "Pre-training layer 2, epoch 82, cost  -45.8605735815\n",
      "Pre-training layer 2, epoch 83, cost  -45.8509578326\n",
      "Pre-training layer 2, epoch 84, cost  -45.8375006664\n",
      "Pre-training layer 2, epoch 85, cost  -45.8234498239\n",
      "Pre-training layer 2, epoch 86, cost  -45.8235703902\n",
      "Pre-training layer 2, epoch 87, cost  -45.8038540689\n",
      "Pre-training layer 2, epoch 88, cost  -45.7972995304\n",
      "Pre-training layer 2, epoch 89, cost  -45.7773048286\n",
      "Pre-training layer 2, epoch 90, cost  -45.777629341\n",
      "Pre-training layer 2, epoch 91, cost  -45.7836199941\n",
      "Pre-training layer 2, epoch 92, cost  -45.7738274293\n",
      "Pre-training layer 2, epoch 93, cost  -45.749599829\n",
      "Pre-training layer 2, epoch 94, cost  -45.7482147095\n",
      "Pre-training layer 2, epoch 95, cost  -45.7282217073\n",
      "Pre-training layer 2, epoch 96, cost  -45.7178733585\n",
      "Pre-training layer 2, epoch 97, cost  -45.717690767\n",
      "Pre-training layer 2, epoch 98, cost  -45.7138308002\n",
      "Pre-training layer 2, epoch 99, cost  -45.7237437846\n",
      "The pretraining code for file ran for 220.79m\n",
      "... getting the finetuning functions\n",
      "... finetuning the model\n",
      "epoch 1, minibatch 5000/5000, validation error 3.160000 %\n",
      "     epoch 1, minibatch 5000/5000, test error of best model 3.340000 %\n",
      "epoch 2, minibatch 5000/5000, validation error 2.490000 %\n",
      "     epoch 2, minibatch 5000/5000, test error of best model 2.640000 %\n",
      "epoch 3, minibatch 5000/5000, validation error 2.190000 %\n",
      "     epoch 3, minibatch 5000/5000, test error of best model 2.370000 %\n",
      "epoch 4, minibatch 5000/5000, validation error 2.090000 %\n",
      "     epoch 4, minibatch 5000/5000, test error of best model 2.170000 %\n",
      "epoch 5, minibatch 5000/5000, validation error 1.920000 %\n",
      "     epoch 5, minibatch 5000/5000, test error of best model 2.070000 %\n",
      "epoch 6, minibatch 5000/5000, validation error 1.910000 %\n",
      "     epoch 6, minibatch 5000/5000, test error of best model 1.960000 %\n",
      "epoch 7, minibatch 5000/5000, validation error 1.820000 %\n",
      "     epoch 7, minibatch 5000/5000, test error of best model 1.910000 %\n",
      "epoch 8, minibatch 5000/5000, validation error 1.780000 %\n",
      "     epoch 8, minibatch 5000/5000, test error of best model 1.880000 %\n",
      "epoch 9, minibatch 5000/5000, validation error 1.730000 %\n",
      "     epoch 9, minibatch 5000/5000, test error of best model 1.870000 %\n",
      "epoch 10, minibatch 5000/5000, validation error 1.710000 %\n",
      "     epoch 10, minibatch 5000/5000, test error of best model 1.750000 %\n",
      "epoch 11, minibatch 5000/5000, validation error 1.650000 %\n",
      "     epoch 11, minibatch 5000/5000, test error of best model 1.660000 %\n",
      "epoch 12, minibatch 5000/5000, validation error 1.590000 %\n",
      "     epoch 12, minibatch 5000/5000, test error of best model 1.630000 %\n",
      "epoch 13, minibatch 5000/5000, validation error 1.550000 %\n",
      "     epoch 13, minibatch 5000/5000, test error of best model 1.600000 %\n",
      "epoch 14, minibatch 5000/5000, validation error 1.520000 %\n",
      "     epoch 14, minibatch 5000/5000, test error of best model 1.620000 %\n",
      "epoch 15, minibatch 5000/5000, validation error 1.500000 %\n",
      "     epoch 15, minibatch 5000/5000, test error of best model 1.610000 %\n",
      "epoch 16, minibatch 5000/5000, validation error 1.480000 %\n",
      "     epoch 16, minibatch 5000/5000, test error of best model 1.630000 %\n",
      "epoch 17, minibatch 5000/5000, validation error 1.500000 %\n",
      "epoch 18, minibatch 5000/5000, validation error 1.510000 %\n",
      "epoch 19, minibatch 5000/5000, validation error 1.510000 %\n",
      "epoch 20, minibatch 5000/5000, validation error 1.510000 %\n",
      "epoch 21, minibatch 5000/5000, validation error 1.490000 %\n",
      "epoch 22, minibatch 5000/5000, validation error 1.470000 %\n",
      "     epoch 22, minibatch 5000/5000, test error of best model 1.520000 %\n",
      "epoch 23, minibatch 5000/5000, validation error 1.450000 %\n",
      "     epoch 23, minibatch 5000/5000, test error of best model 1.510000 %\n",
      "epoch 24, minibatch 5000/5000, validation error 1.430000 %\n",
      "     epoch 24, minibatch 5000/5000, test error of best model 1.510000 %\n",
      "epoch 25, minibatch 5000/5000, validation error 1.420000 %\n",
      "     epoch 25, minibatch 5000/5000, test error of best model 1.470000 %\n",
      "epoch 26, minibatch 5000/5000, validation error 1.420000 %\n",
      "epoch 27, minibatch 5000/5000, validation error 1.440000 %\n",
      "epoch 28, minibatch 5000/5000, validation error 1.430000 %\n",
      "epoch 29, minibatch 5000/5000, validation error 1.420000 %\n",
      "epoch 30, minibatch 5000/5000, validation error 1.420000 %\n",
      "epoch 31, minibatch 5000/5000, validation error 1.410000 %\n",
      "     epoch 31, minibatch 5000/5000, test error of best model 1.440000 %\n",
      "epoch 32, minibatch 5000/5000, validation error 1.420000 %\n",
      "epoch 33, minibatch 5000/5000, validation error 1.440000 %\n",
      "epoch 34, minibatch 5000/5000, validation error 1.440000 %\n",
      "epoch 35, minibatch 5000/5000, validation error 1.440000 %\n",
      "epoch 36, minibatch 5000/5000, validation error 1.440000 %\n",
      "epoch 37, minibatch 5000/5000, validation error 1.430000 %\n",
      "epoch 38, minibatch 5000/5000, validation error 1.430000 %\n",
      "epoch 39, minibatch 5000/5000, validation error 1.450000 %\n",
      "epoch 40, minibatch 5000/5000, validation error 1.450000 %\n",
      "epoch 41, minibatch 5000/5000, validation error 1.450000 %\n",
      "epoch 42, minibatch 5000/5000, validation error 1.450000 %\n",
      "epoch 43, minibatch 5000/5000, validation error 1.450000 %\n",
      "epoch 44, minibatch 5000/5000, validation error 1.450000 %\n",
      "epoch 45, minibatch 5000/5000, validation error 1.450000 %\n",
      "epoch 46, minibatch 5000/5000, validation error 1.450000 %\n",
      "epoch 47, minibatch 5000/5000, validation error 1.450000 %\n",
      "epoch 48, minibatch 5000/5000, validation error 1.450000 %\n",
      "epoch 49, minibatch 5000/5000, validation error 1.450000 %\n",
      "epoch 50, minibatch 5000/5000, validation error 1.450000 %\n",
      "epoch 51, minibatch 5000/5000, validation error 1.450000 %\n",
      "epoch 52, minibatch 5000/5000, validation error 1.440000 %\n",
      "epoch 53, minibatch 5000/5000, validation error 1.440000 %\n",
      "epoch 54, minibatch 5000/5000, validation error 1.440000 %\n",
      "epoch 55, minibatch 5000/5000, validation error 1.440000 %\n",
      "epoch 56, minibatch 5000/5000, validation error 1.440000 %\n",
      "epoch 57, minibatch 5000/5000, validation error 1.450000 %\n",
      "epoch 58, minibatch 5000/5000, validation error 1.440000 %\n",
      "epoch 59, minibatch 5000/5000, validation error 1.440000 %\n",
      "epoch 60, minibatch 5000/5000, validation error 1.440000 %\n",
      "epoch 61, minibatch 5000/5000, validation error 1.430000 %\n",
      "Optimization complete with best validation score of 1.410000 %, obtained at iteration 155000, with test performance 1.440000 %\n",
      "The fine tuning code for file ran for 58.32m\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\"\"\"\n",
    "from __future__ import print_function, division\n",
    "import os\n",
    "import sys\n",
    "import timeit\n",
    "\n",
    "import numpy\n",
    "\n",
    "import theano\n",
    "import theano.tensor as T\n",
    "from theano.sandbox.rng_mrg import MRG_RandomStreams\n",
    "\n",
    "from logistic_sgd import LogisticRegression, load_data\n",
    "from mlp import HiddenLayer\n",
    "from rbm import RBM\n",
    "\n",
    "\n",
    "# start-snippet-1\n",
    "class DBN(object):\n",
    "    \"\"\"Deep Belief Network\n",
    "\n",
    "    A deep belief network is obtained by stacking several RBMs on top of each\n",
    "    other. The hidden layer of the RBM at layer `i` becomes the input of the\n",
    "    RBM at layer `i+1`. The first layer RBM gets as input the input of the\n",
    "    network, and the hidden layer of the last RBM represents the output. When\n",
    "    used for classification, the DBN is treated as a MLP, by adding a logistic\n",
    "    regression layer on top.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, numpy_rng, theano_rng=None, n_ins=784,\n",
    "                 hidden_layers_sizes=[500, 500], n_outs=10):\n",
    "        \"\"\"This class is made to support a variable number of layers.\n",
    "\n",
    "        :type numpy_rng: numpy.random.RandomState\n",
    "        :param numpy_rng: numpy random number generator used to draw initial\n",
    "                    weights\n",
    "\n",
    "        :type theano_rng: theano.tensor.shared_randomstreams.RandomStreams\n",
    "        :param theano_rng: Theano random generator; if None is given one is\n",
    "                           generated based on a seed drawn from `rng`\n",
    "\n",
    "        :type n_ins: int\n",
    "        :param n_ins: dimension of the input to the DBN\n",
    "\n",
    "        :type hidden_layers_sizes: list of ints\n",
    "        :param hidden_layers_sizes: intermediate layers size, must contain\n",
    "                               at least one value\n",
    "\n",
    "        :type n_outs: int\n",
    "        :param n_outs: dimension of the output of the network\n",
    "        \"\"\"\n",
    "\n",
    "        self.sigmoid_layers = []\n",
    "        self.rbm_layers = []\n",
    "        self.params = []\n",
    "        self.n_layers = len(hidden_layers_sizes)\n",
    "\n",
    "        assert self.n_layers > 0\n",
    "\n",
    "        if not theano_rng:\n",
    "            theano_rng = MRG_RandomStreams(numpy_rng.randint(2 ** 30))\n",
    "\n",
    "        # allocate symbolic variables for the data\n",
    "\n",
    "        # the data is presented as rasterized images\n",
    "        self.x = T.matrix('x')\n",
    "\n",
    "        # the labels are presented as 1D vector of [int] labels\n",
    "        self.y = T.ivector('y')\n",
    "        # end-snippet-1\n",
    "        # The DBN is an MLP, for which all weights of intermediate\n",
    "        # layers are shared with a different RBM.  We will first\n",
    "        # construct the DBN as a deep multilayer perceptron, and when\n",
    "        # constructing each sigmoidal layer we also construct an RBM\n",
    "        # that shares weights with that layer. During pretraining we\n",
    "        # will train these RBMs (which will lead to chainging the\n",
    "        # weights of the MLP as well) During finetuning we will finish\n",
    "        # training the DBN by doing stochastic gradient descent on the\n",
    "        # MLP.\n",
    "\n",
    "        for i in range(self.n_layers):\n",
    "            # construct the sigmoidal layer\n",
    "\n",
    "            # the size of the input is either the number of hidden\n",
    "            # units of the layer below or the input size if we are on\n",
    "            # the first layer\n",
    "            if i == 0:\n",
    "                input_size = n_ins\n",
    "            else:\n",
    "                input_size = hidden_layers_sizes[i - 1]\n",
    "\n",
    "            # the input to this layer is either the activation of the\n",
    "            # hidden layer below or the input of the DBN if you are on\n",
    "            # the first layer\n",
    "            if i == 0:\n",
    "                layer_input = self.x\n",
    "            else:\n",
    "                layer_input = self.sigmoid_layers[-1].output\n",
    "\n",
    "            sigmoid_layer = HiddenLayer(rng=numpy_rng,\n",
    "                                        input=layer_input,\n",
    "                                        n_in=input_size,\n",
    "                                        n_out=hidden_layers_sizes[i],\n",
    "                                        activation=T.nnet.sigmoid)\n",
    "\n",
    "            # add the layer to our list of layers\n",
    "            self.sigmoid_layers.append(sigmoid_layer)\n",
    "\n",
    "            # its arguably a philosophical question...  but we are\n",
    "            # going to only declare that the parameters of the\n",
    "            # sigmoid_layers are parameters of the DBN. The visible\n",
    "            # biases in the RBM are parameters of those RBMs, but not\n",
    "            # of the DBN.\n",
    "            self.params.extend(sigmoid_layer.params)\n",
    "\n",
    "            # Construct an RBM that shared weights with this layer\n",
    "            rbm_layer = RBM(numpy_rng=numpy_rng,\n",
    "                            theano_rng=theano_rng,\n",
    "                            input=layer_input,\n",
    "                            n_visible=input_size,\n",
    "                            n_hidden=hidden_layers_sizes[i],\n",
    "                            W=sigmoid_layer.W,\n",
    "                            hbias=sigmoid_layer.b)\n",
    "            self.rbm_layers.append(rbm_layer)\n",
    "\n",
    "        # We now need to add a logistic layer on top of the MLP\n",
    "        self.logLayer = LogisticRegression(\n",
    "            input=self.sigmoid_layers[-1].output,\n",
    "            n_in=hidden_layers_sizes[-1],\n",
    "            n_out=n_outs)\n",
    "        self.params.extend(self.logLayer.params)\n",
    "\n",
    "        # compute the cost for second phase of training, defined as the\n",
    "        # negative log likelihood of the logistic regression (output) layer\n",
    "        self.finetune_cost = self.logLayer.negative_log_likelihood(self.y)\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        # symbolic variable that points to the number of errors made on the\n",
    "        # minibatch given by self.x and self.y\n",
    "        self.errors = self.logLayer.errors(self.y)\n",
    "\n",
    "    def pretraining_functions(self, train_set_x, batch_size, k):\n",
    "        '''Generates a list of functions, for performing one step of\n",
    "        gradient descent at a given layer. The function will require\n",
    "        as input the minibatch index, and to train an RBM you just\n",
    "        need to iterate, calling the corresponding function on all\n",
    "        minibatch indexes.\n",
    "\n",
    "        :type train_set_x: theano.tensor.TensorType\n",
    "        :param train_set_x: Shared var. that contains all datapoints used\n",
    "                            for training the RBM\n",
    "        :type batch_size: int\n",
    "        :param batch_size: size of a [mini]batch\n",
    "        :param k: number of Gibbs steps to do in CD-k / PCD-k\n",
    "\n",
    "        '''\n",
    "\n",
    "        # index to a [mini]batch\n",
    "        index = T.lscalar('index')  # index to a minibatch\n",
    "        learning_rate = T.scalar('lr')  # learning rate to use\n",
    "\n",
    "        # begining of a batch, given `index`\n",
    "        batch_begin = index * batch_size\n",
    "        # ending of a batch given `index`\n",
    "        batch_end = batch_begin + batch_size\n",
    "\n",
    "        pretrain_fns = []\n",
    "        for rbm in self.rbm_layers:\n",
    "\n",
    "            # get the cost and the updates list\n",
    "            # using CD-k here (persisent=None) for training each RBM.\n",
    "            # TODO: change cost function to reconstruction error\n",
    "            cost, updates = rbm.get_cost_updates(learning_rate,\n",
    "                                                 persistent=None, k=k)\n",
    "\n",
    "            # compile the theano function\n",
    "            fn = theano.function(\n",
    "                inputs=[index, theano.In(learning_rate, value=0.1)],\n",
    "                outputs=cost,\n",
    "                updates=updates,\n",
    "                givens={\n",
    "                    self.x: train_set_x[batch_begin:batch_end]\n",
    "                }\n",
    "            )\n",
    "            # append `fn` to the list of functions\n",
    "            pretrain_fns.append(fn)\n",
    "\n",
    "        return pretrain_fns\n",
    "\n",
    "    def build_finetune_functions(self, datasets, batch_size, learning_rate):\n",
    "        '''Generates a function `train` that implements one step of\n",
    "        finetuning, a function `validate` that computes the error on a\n",
    "        batch from the validation set, and a function `test` that\n",
    "        computes the error on a batch from the testing set\n",
    "\n",
    "        :type datasets: list of pairs of theano.tensor.TensorType\n",
    "        :param datasets: It is a list that contain all the datasets;\n",
    "                        the has to contain three pairs, `train`,\n",
    "                        `valid`, `test` in this order, where each pair\n",
    "                        is formed of two Theano variables, one for the\n",
    "                        datapoints, the other for the labels\n",
    "        :type batch_size: int\n",
    "        :param batch_size: size of a minibatch\n",
    "        :type learning_rate: float\n",
    "        :param learning_rate: learning rate used during finetune stage\n",
    "\n",
    "        '''\n",
    "\n",
    "        (train_set_x, train_set_y) = datasets[0]\n",
    "        (valid_set_x, valid_set_y) = datasets[1]\n",
    "        (test_set_x, test_set_y) = datasets[2]\n",
    "\n",
    "        # compute number of minibatches for training, validation and testing\n",
    "        n_valid_batches = valid_set_x.get_value(borrow=True).shape[0]\n",
    "        n_valid_batches //= batch_size\n",
    "        n_test_batches = test_set_x.get_value(borrow=True).shape[0]\n",
    "        n_test_batches //= batch_size\n",
    "\n",
    "        index = T.lscalar('index')  # index to a [mini]batch\n",
    "\n",
    "        # compute the gradients with respect to the model parameters\n",
    "        gparams = T.grad(self.finetune_cost, self.params)\n",
    "\n",
    "        # compute list of fine-tuning updates\n",
    "        updates = []\n",
    "        for param, gparam in zip(self.params, gparams):\n",
    "            updates.append((param, param - gparam * learning_rate))\n",
    "\n",
    "        train_fn = theano.function(\n",
    "            inputs=[index],\n",
    "            outputs=self.finetune_cost,\n",
    "            updates=updates,\n",
    "            givens={\n",
    "                self.x: train_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: train_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        test_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: test_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: test_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        valid_score_i = theano.function(\n",
    "            [index],\n",
    "            self.errors,\n",
    "            givens={\n",
    "                self.x: valid_set_x[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ],\n",
    "                self.y: valid_set_y[\n",
    "                    index * batch_size: (index + 1) * batch_size\n",
    "                ]\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Create a function that scans the entire validation set\n",
    "        def valid_score():\n",
    "            return [valid_score_i(i) for i in range(n_valid_batches)]\n",
    "\n",
    "        # Create a function that scans the entire test set\n",
    "        def test_score():\n",
    "            return [test_score_i(i) for i in range(n_test_batches)]\n",
    "\n",
    "        return train_fn, valid_score, test_score\n",
    "\n",
    "\n",
    "def test_DBN(finetune_lr=0.1, pretraining_epochs=100,\n",
    "             pretrain_lr=0.01, k=1, training_epochs=1000,\n",
    "             dataset='mnist.pkl.gz', batch_size=10):\n",
    "    \"\"\"\n",
    "    Demonstrates how to train and test a Deep Belief Network.\n",
    "\n",
    "    This is demonstrated on MNIST.\n",
    "\n",
    "    :type finetune_lr: float\n",
    "    :param finetune_lr: learning rate used in the finetune stage\n",
    "    :type pretraining_epochs: int\n",
    "    :param pretraining_epochs: number of epoch to do pretraining\n",
    "    :type pretrain_lr: float\n",
    "    :param pretrain_lr: learning rate to be used during pre-training\n",
    "    :type k: int\n",
    "    :param k: number of Gibbs steps in CD/PCD\n",
    "    :type training_epochs: int\n",
    "    :param training_epochs: maximal number of iterations ot run the optimizer\n",
    "    :type dataset: string\n",
    "    :param dataset: path the the pickled dataset\n",
    "    :type batch_size: int\n",
    "    :param batch_size: the size of a minibatch\n",
    "    \"\"\"\n",
    "\n",
    "    datasets = load_data(dataset)\n",
    "\n",
    "    train_set_x, train_set_y = datasets[0]\n",
    "    valid_set_x, valid_set_y = datasets[1]\n",
    "    test_set_x, test_set_y = datasets[2]\n",
    "\n",
    "    # compute number of minibatches for training, validation and testing\n",
    "    n_train_batches = train_set_x.get_value(borrow=True).shape[0] // batch_size\n",
    "\n",
    "    # numpy random generator\n",
    "    numpy_rng = numpy.random.RandomState(123)\n",
    "    print('... building the model')\n",
    "    # construct the Deep Belief Network\n",
    "    dbn = DBN(numpy_rng=numpy_rng, n_ins=28 * 28,\n",
    "              hidden_layers_sizes=[1000, 1000, 1000],\n",
    "              n_outs=10)\n",
    "\n",
    "    # start-snippet-2\n",
    "    #########################\n",
    "    # PRETRAINING THE MODEL #\n",
    "    #########################\n",
    "    print('... getting the pretraining functions')\n",
    "    pretraining_fns = dbn.pretraining_functions(train_set_x=train_set_x,\n",
    "                                                batch_size=batch_size,\n",
    "                                                k=k)\n",
    "\n",
    "    print('... pre-training the model')\n",
    "    start_time = timeit.default_timer()\n",
    "    # Pre-train layer-wise\n",
    "    for i in range(dbn.n_layers):\n",
    "        # go through pretraining epochs\n",
    "        for epoch in range(pretraining_epochs):\n",
    "            # go through the training set\n",
    "            c = []\n",
    "            for batch_index in range(n_train_batches):\n",
    "                c.append(pretraining_fns[i](index=batch_index,\n",
    "                                            lr=pretrain_lr))\n",
    "            print('Pre-training layer %i, epoch %d, cost ' % (i, epoch), end=' ')\n",
    "            print(numpy.mean(c, dtype='float64'))\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    # end-snippet-2\n",
    "    print('The pretraining code for file ran for %.2fm' % ((end_time - start_time) / 60.))\n",
    "    ########################\n",
    "    # FINETUNING THE MODEL #\n",
    "    ########################\n",
    "\n",
    "    # get the training, validation and testing function for the model\n",
    "    print('... getting the finetuning functions')\n",
    "    train_fn, validate_model, test_model = dbn.build_finetune_functions(\n",
    "        datasets=datasets,\n",
    "        batch_size=batch_size,\n",
    "        learning_rate=finetune_lr\n",
    "    )\n",
    "\n",
    "    print('... finetuning the model')\n",
    "    # early-stopping parameters\n",
    "\n",
    "    # look as this many examples regardless\n",
    "    patience = 4 * n_train_batches\n",
    "\n",
    "    # wait this much longer when a new best is found\n",
    "    patience_increase = 2.\n",
    "\n",
    "    # a relative improvement of this much is considered significant\n",
    "    improvement_threshold = 0.995\n",
    "\n",
    "    # go through this many minibatches before checking the network on\n",
    "    # the validation set; in this case we check every epoch\n",
    "    validation_frequency = min(n_train_batches, patience / 2)\n",
    "\n",
    "    best_validation_loss = numpy.inf\n",
    "    test_score = 0.\n",
    "    start_time = timeit.default_timer()\n",
    "\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "\n",
    "    while (epoch < training_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "\n",
    "            train_fn(minibatch_index)\n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index\n",
    "\n",
    "            if (iter + 1) % validation_frequency == 0:\n",
    "\n",
    "                validation_losses = validate_model()\n",
    "                this_validation_loss = numpy.mean(validation_losses, dtype='float64')\n",
    "                print('epoch %i, minibatch %i/%i, validation error %f %%' % (\n",
    "                    epoch,\n",
    "                    minibatch_index + 1,\n",
    "                    n_train_batches,\n",
    "                    this_validation_loss * 100.\n",
    "                    )\n",
    "                )\n",
    "\n",
    "                # if we got the best validation score until now\n",
    "                if this_validation_loss < best_validation_loss:\n",
    "\n",
    "                    # improve patience if loss improvement is good enough\n",
    "                    if (this_validation_loss < best_validation_loss *\n",
    "                            improvement_threshold):\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "\n",
    "                    # save best validation score and iteration number\n",
    "                    best_validation_loss = this_validation_loss\n",
    "                    best_iter = iter\n",
    "\n",
    "                    # test it on the test set\n",
    "                    test_losses = test_model()\n",
    "                    test_score = numpy.mean(test_losses, dtype='float64')\n",
    "                    print(('     epoch %i, minibatch %i/%i, test error of '\n",
    "                           'best model %f %%') %\n",
    "                          (epoch, minibatch_index + 1, n_train_batches,\n",
    "                          test_score * 100.))\n",
    "\n",
    "            if patience <= iter:\n",
    "                done_looping = True\n",
    "                break\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "    print(('Optimization complete with best validation score of %f %%, ''obtained at iteration %i, ''with test performance %f %%') % (best_validation_loss * 100., best_iter + 1, test_score * 100.))\n",
    "    print('The fine tuning code for file ran for %.2fm' % ((end_time - start_time) / 60.))\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    test_DBN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
